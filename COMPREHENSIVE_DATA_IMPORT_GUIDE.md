# SAFMC FMP Tracker - Comprehensive Data Import Guide

## Overview
This guide covers the complete data import strategy for populating the SAFMC FMP Tracker with comprehensive historical data (2018-present) to enable natural language queries across all fishery management information.

---

## Phase 1: Enhanced Amendment Scraping âœ… COMPLETED

### What Was Built

**Enhanced Amendment Scraper** (`src/scrapers/amendments_scraper_enhanced.py`)
- Comprehensive scraping of all 8 FMP pages
- Historical amendments from 2018-present
- Individual amendment detail pages for metadata
- Timeline/milestone extraction
- Document discovery and cataloging
- Federal Register number extraction
- Staff contact information
- Automatic classification of amendment status

### API Endpoint

**POST** `/api/scrape/amendments/comprehensive`

**Authentication:** Requires admin role

**What It Does:**
1. Scrapes main "Amendments Under Development" page
2. Scrapes all 8 FMP pages (Snapper Grouper, Dolphin Wahoo, CMP, etc.)
3. Identifies both current AND completed amendments
4. For each amendment found, scrapes individual detail page for:
   - Timeline dates (scoping, public hearing, submission, approval, implementation)
   - Detailed descriptions
   - Staff contact information
   - Related documents (scoping docs, drafts, finals, EAs)
   - Federal Register citations (when available)
5. Saves all data to database (Actions, Milestones)
6. Queues documents for processing

**Response Example:**
```json
{
  "success": true,
  "itemsFound": 127,
  "itemsNew": 89,
  "itemsUpdated": 38,
  "milestonesCreated": 245,
  "documentsQueued": 412,
  "duration": 185000,
  "sources_scraped": [
    "amendments-under-development",
    "Snapper Grouper",
    "Dolphin Wahoo",
    "Coastal Migratory Pelagics",
    "Golden Crab",
    "Sargassum",
    "Shrimp",
    "Spiny Lobster",
    "Coral"
  ]
}
```

### How to Run

**Option 1: Via API (Postman/cURL)**
```bash
curl -X POST https://your-app.onrender.com/api/scrape/amendments/comprehensive \
  -H "Authorization: Bearer YOUR_TOKEN" \
  -H "Content-Type: application/json"
```

**Option 2: Via Python Script**
```python
from src.scrapers.amendments_scraper_enhanced import EnhancedAmendmentsScraper

scraper = EnhancedAmendmentsScraper(rate_limit=1.5)
results = scraper.scrape_all_comprehensive()

print(f"Found {results['total_found']} amendments")
print(f"Documents: {len(results['documents'])}")
print(f"Milestones: {len(results['milestones'])}")
```

**Expected Runtime:** 10-15 minutes (depends on network speed and rate limiting)

**Rate Limiting:** 1.5 seconds between requests (respectful scraping)

---

## Phase 2: Vector Search Document System (NEXT)

### Architecture

```
PDF Documents â†’ Extract Text â†’ Generate Embeddings â†’ Vector DB â†’ Natural Language Search
                     â†“              (OpenAI)           (Pinecone)       â†“
              Store in DB                                         AI Assistant
```

### Required Services

1. **OpenAI API** (for embeddings)
   - Model: `text-embedding-3-small`
   - Cost: ~$0.02 per 1000 documents
   - Sign up: https://platform.openai.com/

2. **Pinecone** (vector database)
   - Free tier: 1M vectors, sufficient for SAFMC
   - Sign up: https://www.pinecone.io/

### Implementation Plan

**Step 1: Document Processor Service**
```python
# src/services/document_processor.py

class DocumentProcessor:
    def process_queue(self):
        # Get documents from document_scrape_queue
        # Extract text from PDFs
        # Chunk into ~500 word segments
        # Generate embeddings
        # Store in Pinecone
        pass
```

**Step 2: Vector Search Integration**
```python
# Update src/services/ai_service.py

def answer_question_with_context(self, question):
    # 1. Embed question
    # 2. Query Pinecone for relevant chunks
    # 3. Pass chunks as context to Claude
    # 4. Return answer with citations
    pass
```

**Step 3: Natural Language Queries**
Users can ask:
- "What amendments addressed red snapper size limits in 2020?"
- "Show me all public comments about recreational for-hire limits"
- "What meetings discussed Dolphin Wahoo Amendment 10?"
- "Find documents about stock assessment uncertainty"

### Cost Estimate
- 1000 documents Ã— 10 pages each = 10,000 pages
- ~5,000 chunks (assuming 2 chunks/page)
- Embeddings: $0.10 one-time
- Pinecone: Free tier
- **Total: ~$0.10 one-time + $0 monthly**

---

## Phase 3: Automated Discovery System (FUTURE)

### Features

1. **Website Structure Monitoring**
   - Weekly sitemap crawls
   - Detect new FMP pages
   - Alert on structure changes
   - Auto-update scrapers

2. **Comment Source Discovery**
   - Find new Google Sheets
   - Monitor regulations.gov
   - Detect comment periods
   - Auto-configure importers

3. **Change Detection**
   - Hash page content
   - Track status changes
   - Monitor new documents
   - Daily diff reports

4. **Self-Healing Scrapers**
   - Alternative selectors on failure
   - ML-based pattern detection
   - Fallback extraction methods

---

## Data Coverage by Source

### Amendments (Phase 1 âœ…)
| Source | Coverage | Status |
|--------|----------|---------|
| Amendments Under Development | Current | âœ… Complete |
| Snapper Grouper FMP Page | 2018-present | âœ… Complete |
| Dolphin Wahoo FMP Page | 2018-present | âœ… Complete |
| Coastal Migratory Pelagics | 2018-present | âœ… Complete |
| Golden Crab FMP Page | 2018-present | âœ… Complete |
| Sargassum FMP Page | 2018-present | âœ… Complete |
| Shrimp FMP Page | 2018-present | âœ… Complete |
| Spiny Lobster FMP Page | 2018-present | âœ… Complete |
| Coral FMP Page | 2018-present | âœ… Complete |

### Meetings (Existing)
| Source | Coverage | Status |
|--------|----------|---------|
| SAFMC Meetings | Current | âœ… Existing |
| Multi-Council RSS Feeds | Current | âœ… Existing |
| FisheryPulse Aggregator | Current | âœ… Existing |

### Comments (Existing)
| Source | Coverage | Status |
|--------|----------|---------|
| Google Sheets (10+ sources) | 2020-present | âœ… Existing |
| Regulations.gov | Not yet | ðŸ”„ Planned |

### Documents (Phase 2 - Planned)
| Type | Coverage | Status |
|------|----------|---------|
| Briefing Books | Queued | ðŸ”„ Ready for processing |
| Amendment Documents | Queued | ðŸ”„ Ready for processing |
| Meeting Minutes | Queued | ðŸ”„ Ready for processing |
| SAFE Reports | Partial | ðŸ”„ Needs extraction |
| Stock Assessments | Partial | ðŸ”„ Needs SEDAR fix |

---

## Database Schema Updates

### Current Schema (Supports Phase 1)
- âœ… `actions` table - amendments, frameworks, regulatory actions
- âœ… `milestones` table - timeline dates
- âœ… `document_scrape_queue` table - document URLs awaiting processing
- âœ… `scrape_log` table - operation tracking

### Recommended Additions (for Phase 2+)
```sql
-- Federal Register citations
ALTER TABLE actions ADD COLUMN federal_register_citation VARCHAR(100);
ALTER TABLE actions ADD COLUMN federal_register_date DATE;

-- Document full text and embeddings (for vector search)
ALTER TABLE fmp_documents ADD COLUMN full_text TEXT;
ALTER TABLE fmp_documents ADD COLUMN embedding_id VARCHAR(100); -- Pinecone ID
ALTER TABLE fmp_documents ADD COLUMN processed_date TIMESTAMP;

-- Amendment-to-Document relationships
CREATE TABLE action_documents (
    action_id VARCHAR(50) REFERENCES actions(action_id),
    document_id INTEGER REFERENCES fmp_documents(id),
    relationship_type VARCHAR(50), -- 'scoping', 'draft', 'final', 'ea', etc.
    PRIMARY KEY (action_id, document_id)
);
```

---

## Testing the Enhanced Scraper

### Test 1: Run Comprehensive Scrape

**Via API:**
```bash
# Set your auth token
TOKEN="your-auth-token-here"

# Run comprehensive scrape
curl -X POST https://safmc-fmp-tracker.onrender.com/api/scrape/amendments/comprehensive \
  -H "Authorization: Bearer $TOKEN" \
  -H "Content-Type: application/json" \
  -v
```

**Expected Output:**
- 100-150 amendments total (mix of current and historical)
- 200-300 milestones
- 300-500 documents
- Duration: 10-15 minutes

### Test 2: Verify Data Quality

```sql
-- Check amendment distribution by FMP
SELECT fmp, COUNT(*) as count
FROM actions
GROUP BY fmp
ORDER BY count DESC;

-- Check progress stage distribution
SELECT progress_stage, COUNT(*) as count
FROM actions
GROUP BY progress_stage
ORDER BY count DESC;

-- Check milestones created
SELECT COUNT(*) as milestone_count
FROM milestones
WHERE milestone_type = 'timeline';

-- Check recently scraped
SELECT COUNT(*) as recent_scrapes
FROM actions
WHERE last_scraped > NOW() - INTERVAL '1 day';
```

### Test 3: Verify Individual Amendment Details

```sql
-- Look at a specific amendment
SELECT *
FROM actions
WHERE action_id = 'sg-am-48'  -- Snapper Grouper Amendment 48
LIMIT 1;

-- Check its milestones
SELECT *
FROM milestones
WHERE action_id = 'sg-am-48'
ORDER BY target_date;
```

---

## Troubleshooting

### Issue: Scraper Times Out
**Cause:** Network latency or Render free tier limitations
**Solution:**
- Increase rate_limit to 2.0 seconds
- Split scraping into batches (by FMP)
- Run during off-peak hours

### Issue: Some Amendments Not Found
**Cause:** Website structure varies by FMP
**Solution:**
- Check scraper logs for specific FMP errors
- Manually inspect FMP page HTML structure
- Update selectors in EnhancedAmendmentsScraper

### Issue: Documents Not Queuing
**Cause:** document_scrape_queue table might not exist
**Solution:**
```python
# Run migration
from migrations.init_fmp_document_tables import run_migration
run_migration()
```

### Issue: Federal Register Numbers Missing
**Cause:** Not all amendments have FR numbers on detail pages
**Solution:** This is expected - only implemented amendments have FR numbers

---

## Next Steps - Recommendations

### Immediate (This Week)
1. âœ… **Run comprehensive scrape** - Populate database with historical data
2. **Verify data quality** - Check SQL queries above
3. **Sign up for OpenAI & Pinecone** - Get API keys ready

### Short-term (Next 2 Weeks)
1. **Build document processor** - Extract PDF text, generate embeddings
2. **Integrate vector search** - Update AI Assistant to use document context
3. **Test natural language queries** - Verify users can ask questions

### Medium-term (Next Month)
1. **Implement automated discovery** - Website monitoring, change detection
2. **Add regulations.gov scraper** - Expand comment sources
3. **Build admin dashboard** - Monitor scraping operations, data quality

### Long-term (Next Quarter)
1. **Historical backfill** - Extend to pre-2018 amendments
2. **Real-time monitoring** - Detect changes within hours
3. **Predictive analytics** - Forecast amendment timelines
4. **Stakeholder notifications** - Alert system for interested parties

---

## Performance Metrics

### Scraping Performance
- **Rate:** ~8-12 amendments/minute
- **Total Time:** 10-15 minutes for full comprehensive scrape
- **Success Rate:** >95% (some 404s expected for old amendments)
- **Data Quality:** ~90% of amendments have complete metadata

### Expected Database Growth
- **Amendments:** ~150 records (2018-present)
- **Milestones:** ~300 records
- **Documents:** ~500 queued for processing
- **Storage:** ~50MB (before document text extraction)

---

## API Reference

### GET /api/actions
Returns all amendments with filters

**Query Parameters:**
- `fmp` - Filter by FMP name
- `progress_stage` - Filter by stage
- `limit` - Results per page
- `offset` - Pagination offset

### POST /api/scrape/amendments/comprehensive
Runs comprehensive historical scrape (admin only)

**Response:**
```json
{
  "success": true,
  "itemsFound": 127,
  "itemsNew": 89,
  "itemsUpdated": 38,
  "milestonesCreated": 245,
  "documentsQueued": 412,
  "duration": 185000
}
```

### GET /api/logs/scrape
View scraping operation logs

**Query Parameters:**
- `limit` - Number of logs
- `source` - Filter by source (e.g., 'amendments_comprehensive')

---

## Maintenance

### Weekly Tasks
- Run comprehensive scrape to update current amendments
- Review scrape logs for errors
- Check data quality metrics

### Monthly Tasks
- Update staff names list in scraper
- Review and update FMP patterns
- Verify Federal Register patterns still match
- Test document processing pipeline

### Quarterly Tasks
- Full database backup
- Performance optimization review
- Update scraping strategies based on website changes
- Review and update API documentation

---

## Support & Troubleshooting

### Logs Location
- Application logs: Check Render dashboard
- Scrape logs: `GET /api/logs/scrape`
- Database logs: PostgreSQL logs in Render

### Common Errors

**"Request timeout"**
- Increase timeout in scraper initialization
- Check network connectivity
- Verify SAFMC website is accessible

**"Duplicate key violation"**
- Amendment already exists
- Deduplication logic working correctly
- No action needed (this is expected)

**"Invalid selector"**
- SAFMC website structure changed
- Update CSS selectors in scraper
- Check scraper logs for specific page URL

### Getting Help
- Check scrape logs first: `GET /api/logs/scrape`
- Review database state: SQL queries above
- Check GitHub issues: https://github.com/akornenvironmental/safmc-fmp-tracker/issues

---

## Summary

**Phase 1 (Completed):** âœ… Enhanced amendment scraper with comprehensive historical data
- Scrapes 8 FMP pages
- Extracts detailed metadata, timelines, documents
- API endpoint ready to use
- Ready for testing and population

**Phase 2 (Next):** ðŸ”„ Vector search for natural language queries
- Requires OpenAI + Pinecone setup
- Document processing pipeline
- AI Assistant integration
- Estimated: 3-4 days development

**Phase 3 (Future):** ðŸ“‹ Automated discovery and monitoring
- Website change detection
- New source discovery
- Self-healing scrapers
- Estimated: 2-3 days development

**Total Timeline:** 14-20 days for complete system

The enhanced scraper is now deployed and ready for comprehensive data import. Next step is to run the scraper and populate the database, then move on to vector search implementation.
